"""
RAG ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ ë„êµ¬ (evaluate.py)

WebUIì™€ ë™ì¼í•œ RAG ë°©ì‹ ë° ë©”ëª¨ë¦¬ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬
RAGAS í”„ë ˆì„ì›Œí¬ë¡œ ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- WebUIì™€ ë™ì¼í•œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (VectorStoreManager, LLMManager, RetrieverManager, ChatHistoryManager)
- ì‚¬ì „ ì •ì˜ëœ ì§ˆë¬¸-ì •ë‹µ ë°ì´í„°ì…‹ ì‚¬ìš© (data/eval/question_dataset.json)
- RAGAS ë©”íŠ¸ë¦­ì„ í†µí•œ í’ˆì§ˆ í‰ê°€ (faithfulness, answer_relevancy, context_recall, answer_correctness)
- í‰ê°€ ê²°ê³¼ ì €ì¥ ë° ë¦¬í¬íŠ¸ ìƒì„± (data/eval/evaluation_results/)
- baseline.py ë°©ì‹ì˜ Upstage API ì§ì ‘ ì‚¬ìš©ìœ¼ë¡œ RAGAS í˜¸í™˜ì„± í™•ë³´

ì‚¬ìš©ë²•:
    uv run python code/evaluate.py

Author: AI Assistant
Date: 2025-08-22
"""

import os
import sys
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Tuple

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
script_dir = Path(__file__).parent.absolute()
if str(script_dir) not in sys.path:
    sys.path.insert(0, str(script_dir))

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv(script_dir / '.env')

# LangChain ë° Upstage imports
from langchain_upstage import UpstageEmbeddings
from langchain.memory import ConversationBufferWindowMemory

# RAGAS imports (Upstage API ì‚¬ìš©)
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy, 
    context_recall,
    answer_correctness
)
from datasets import Dataset

# ëª¨ë“ˆ imports
from modules import (
    VectorStoreManager, LLMManager, RetrieverManager, 
    ChatHistoryManager, LoggerManager
)


class RAGEvaluator:
    """RAG ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """RAGEvaluator ì´ˆê¸°í™”"""
        self.logger = LoggerManager("RAGEvaluator")
        self.project_root = script_dir.parent
        self.dataset_path = self.project_root / "data" / "eval" / "question_dataset.json"
        self.results_dir = self.project_root / "data" / "eval" / "evaluation_results"
        
        # RAGASì—ì„œ Upstage API ì‚¬ìš©í•˜ë„ë¡ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        self._setup_upstage_for_ragas()
        
        # ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸
        self.vector_manager = None
        self.llm_manager = None
        self.retriever_manager = None
        self.chat_history_manager = None
        
        self.logger.log_success("RAG Evaluator ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_upstage_for_ragas(self):
        """RAGASì—ì„œ Upstage APIë¥¼ ì‚¬ìš©í•˜ë„ë¡ í™˜ê²½ë³€ìˆ˜ ì„¤ì •"""
        try:
            # OpenAI í™˜ê²½ë³€ìˆ˜ë¥¼ Upstage APIë¡œ ì„¤ì •
            upstage_api_key = os.getenv("UPSTAGE_API_KEY")
            if upstage_api_key:
                os.environ["OPENAI_API_KEY"] = upstage_api_key
                os.environ["OPENAI_BASE_URL"] = "https://api.upstage.ai/v1"
                
                # RAGASê°€ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ ëª¨ë¸ì„ Upstage ëª¨ë¸ë¡œ ë§¤í•‘
                # baseline.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ëª…ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •
                os.environ["OPENAI_MODEL_NAME"] = "solar-pro2"
                
                self.logger.log_step("RAGAS Upstage ì„¤ì •", "OpenAI í™˜ê²½ë³€ìˆ˜ë¥¼ Upstage APIë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ (ëª¨ë¸: solar-pro2)")
            else:
                self.logger.log_warning("UPSTAGE_API_KEY not found", "RAGAS í‰ê°€ì— ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        except Exception as e:
            self.logger.log_error("RAGAS Upstage ì„¤ì •", e)
    
    def load_evaluation_dataset(self) -> Dict[str, Any]:
        """í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ"""
        self.logger.log_function_start("load_evaluation_dataset")
        
        try:
            with open(self.dataset_path, 'r', encoding='utf-8') as f:
                dataset = json.load(f)
            
            self.logger.log_function_end("load_evaluation_dataset", 
                                       f"{dataset['metadata']['total_questions']}ê°œ ì§ˆë¬¸ ë¡œë“œ")
            return dataset
        
        except Exception as e:
            self.logger.log_error("load_evaluation_dataset", e)
            raise
    
    def initialize_system(self) -> bool:
        """WebUIì™€ ë™ì¼í•œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.logger.log_function_start("initialize_system")
        
        try:
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            self.logger.log_step("ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”")
            embeddings = UpstageEmbeddings(
                api_key=os.getenv("UPSTAGE_API_KEY"),
                model="embedding-query"
            )
            
            # ë²¡í„°ìŠ¤í† ì–´ ê´€ë¦¬ì ì´ˆê¸°í™” (ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©)
            self.logger.log_step("ë²¡í„°ìŠ¤í† ì–´ ê´€ë¦¬ì ì´ˆê¸°í™”")
            pdf_dir = str(self.project_root / "data" / "pdf")
            vectorstore_dir = str(self.project_root / "data" / "vectorstore")
            
            self.vector_manager = VectorStoreManager(
                pdf_dir=pdf_dir,
                vectorstore_dir=vectorstore_dir, 
                embeddings=embeddings,
                chunk_size=1000,
                chunk_overlap=50
            )
            
            # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ/ìƒì„±
            self.logger.log_step("ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ/ìƒì„±")
            vectorstore = self.vector_manager.get_or_create_vectorstore()
            
            if vectorstore is None:
                self.logger.log_error_with_icon("ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # LLM ê´€ë¦¬ì ì´ˆê¸°í™”
            self.logger.log_step("LLM ê´€ë¦¬ì ì´ˆê¸°í™”")
            self.llm_manager = LLMManager()
            
            # ê²€ìƒ‰ê¸° ê´€ë¦¬ì ì´ˆê¸°í™”
            self.logger.log_step("ê²€ìƒ‰ê¸° ê´€ë¦¬ì ì´ˆê¸°í™”")
            self.retriever_manager = RetrieverManager(vectorstore=vectorstore)
            
            # ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ë§Œ, DB ì €ì¥ ì•ˆí•¨)
            self.logger.log_step("ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”")
            self.chat_history_manager = ChatHistoryManager(
                session_id=None,
                sql_manager=None,
                auto_save=False
            )
            
            self.logger.log_function_end("initialize_system", "ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.log_error("initialize_system", e)
            return False
    
    def process_questions(self, dataset: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì§ˆë¬¸ë“¤ì„ ì²˜ë¦¬í•˜ê³  ë‹µë³€ ìƒì„±"""
        self.logger.log_function_start("process_questions", 
                                     count=len(dataset["questions"]))
        
        results = []
        questions = dataset["questions"]
        
        for i, question_data in enumerate(questions, 1):
            self.logger.log_step(f"ì§ˆë¬¸ {i}/{len(questions)} ì²˜ë¦¬", 
                               question_data["question"][:50] + "...")
            
            start_time = time.time()
            
            try:
                # ì§ˆë¬¸ ì²˜ë¦¬ (WebUIì™€ ë™ì¼í•œ íë¦„)
                question = question_data["question"]
                
                # 1. ë¬¸ì„œ ê²€ìƒ‰
                documents = self.retriever_manager.search_documents(question)
                context = self.retriever_manager.format_documents_for_context(documents)
                
                # 2. ì±„íŒ… íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
                chat_history = self.chat_history_manager.get_chat_history_as_dicts()
                
                # 3. LLM ì‘ë‹µ ìƒì„±
                response = self.llm_manager.generate_response(
                    question=question,
                    context=context,
                    chat_history=chat_history
                )
                
                # 4. ë©”ëª¨ë¦¬ì— ëŒ€í™” ì¶”ê°€ (DB ì €ì¥ ì•ˆí•¨)
                self.chat_history_manager.add_conversation_pair(question, response)
                
                # 5. ê²€ìƒ‰ëœ ë¬¸ì„œ ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
                source_documents = self.retriever_manager.get_unique_sources(documents)
                contexts = [doc.page_content for doc in documents]
                
                processing_time = (time.time() - start_time) * 1000  # ms
                
                # ê²°ê³¼ ì €ì¥
                result = {
                    "question_id": question_data["id"],
                    "question": question,
                    "generated_answer": response,
                    "ground_truth": question_data["ground_truth"],
                    "category": question_data["category"],
                    "difficulty": question_data["difficulty"],
                    "retrieved_contexts": contexts,
                    "source_documents": source_documents,
                    "expected_sources": question_data.get("expected_sources", []),
                    "processing_time_ms": round(processing_time, 2),
                    "depends_on": question_data.get("depends_on"),
                    "keywords": question_data.get("keywords", [])
                }
                
                results.append(result)
                
                self.logger.log_success(f"ì§ˆë¬¸ {i} ì²˜ë¦¬ ì™„ë£Œ ({processing_time:.0f}ms)")
                
            except Exception as e:
                self.logger.log_error(f"ì§ˆë¬¸ {i} ì²˜ë¦¬", e)
                # ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ë¹ˆ ê²°ê³¼ ì¶”ê°€
                results.append({
                    "question_id": question_data["id"],
                    "question": question_data["question"],
                    "generated_answer": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                    "ground_truth": question_data["ground_truth"],
                    "category": question_data["category"],
                    "difficulty": question_data["difficulty"],
                    "retrieved_contexts": [],
                    "source_documents": [],
                    "expected_sources": question_data.get("expected_sources", []),
                    "processing_time_ms": 0,
                    "error": str(e)
                })
        
        self.logger.log_function_end("process_questions", f"{len(results)}ê°œ ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ")
        return results
    
    def run_ragas_evaluation(self, results: List[Dict[str, Any]]) -> Dict[str, float]:
        """RAGAS ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•œ í‰ê°€ ì‹¤í–‰ (Upstage API ì‚¬ìš©)"""
        self.logger.log_function_start("run_ragas_evaluation")
        
        try:
            # RAGAS í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ êµ¬ì„±
            dataset_dict = {
                "question": [r["question"] for r in results],
                "answer": [r["generated_answer"] for r in results],
                "contexts": [r["retrieved_contexts"] for r in results],
                "ground_truth": [r["ground_truth"] for r in results]
            }
            
            # Dataset ê°ì²´ ìƒì„±
            dataset = Dataset.from_dict(dataset_dict)
            
            self.logger.log_step("RAGAS í‰ê°€ ì‹¤í–‰", "ë©”íŠ¸ë¦­: faithfulness, answer_relevancy, context_recall, answer_correctness (Upstage API ì‚¬ìš©)")
            
            # baseline.py ë°©ì‹ìœ¼ë¡œ Upstage ëª¨ë¸ ì§ì ‘ ì‚¬ìš©
            from langchain_upstage import ChatUpstage, UpstageEmbeddings
            
            upstage_llm = ChatUpstage(
                api_key=os.getenv("UPSTAGE_API_KEY"),
                model="solar-pro2",
                reasoning_effort="high"
            )
            
            upstage_embeddings = UpstageEmbeddings(
                api_key=os.getenv("UPSTAGE_API_KEY"),
                model="embedding-query"
            )
            
            # RAGAS í‰ê°€ ì‹¤í–‰ (baseline.pyì™€ ë™ì¼í•œ Upstage ëª¨ë¸ ì‚¬ìš©)
            evaluation_result = evaluate(
                dataset=dataset,
                metrics=[faithfulness, answer_relevancy, context_recall, answer_correctness],
                llm=upstage_llm,
                embeddings=upstage_embeddings
            )
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (baseline.py ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •)
            scores = {}
            scores_dict = evaluation_result._scores_dict
            
            for metric_name in ["faithfulness", "answer_relevancy", "context_recall", "answer_correctness"]:
                if metric_name in scores_dict and len(scores_dict[metric_name]) > 0:
                    # ë¦¬ìŠ¤íŠ¸ì˜ í‰ê· ê°’ ê³„ì‚°
                    score_values = scores_dict[metric_name]
                    if isinstance(score_values, list):
                        # numpy scalarì´ë‚˜ float ì²˜ë¦¬
                        avg_score = sum(float(v.item()) if hasattr(v, 'item') else float(v) for v in score_values) / len(score_values)
                        scores[metric_name] = avg_score
                    else:
                        scores[metric_name] = float(score_values.item()) if hasattr(score_values, 'item') else float(score_values)
                else:
                    scores[metric_name] = 0.0
            
            # RAGAS ì¢…í•© ì ìˆ˜ ê³„ì‚° (í‰ê· )
            scores["ragas_score"] = sum(scores.values()) / len(scores)
            
            self.logger.log_function_end("run_ragas_evaluation", f"í‰ê°€ ì™„ë£Œ: {scores['ragas_score']:.3f}")
            return scores
            
        except Exception as e:
            self.logger.log_error("run_ragas_evaluation", e)
            # í‰ê°€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "faithfulness": 0.0,
                "answer_relevancy": 0.0,
                "context_recall": 0.0,
                "answer_correctness": 0.0,
                "ragas_score": 0.0,
                "error": str(e)
            }
    
    def calculate_category_scores(self, results: List[Dict[str, Any]], 
                                overall_scores: Dict[str, float]) -> Dict[str, Any]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°"""
        self.logger.log_function_start("calculate_category_scores")
        
        categories = {}
        
        for result in results:
            category = result["category"]
            if category not in categories:
                categories[category] = {
                    "questions": [],
                    "question_count": 0
                }
            
            categories[category]["questions"].append(result)
            categories[category]["question_count"] += 1
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ (ì „ì²´ ì ìˆ˜ ê¸°ë°˜ ì¶”ì •)
        category_scores = {}
        for category, data in categories.items():
            category_scores[category] = {
                "question_count": data["question_count"],
                "avg_processing_time_ms": sum(q.get("processing_time_ms", 0) 
                                            for q in data["questions"]) / data["question_count"],
                "success_rate": len([q for q in data["questions"] 
                                   if not q.get("error")]) / data["question_count"]
            }
        
        self.logger.log_function_end("calculate_category_scores", 
                                   f"{len(categories)}ê°œ ì¹´í…Œê³ ë¦¬ ë¶„ì„")
        return category_scores
    
    def save_evaluation_results(self, dataset: Dict[str, Any], results: List[Dict[str, Any]], 
                              overall_scores: Dict[str, float], category_scores: Dict[str, Any]) -> str:
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        self.logger.log_function_start("save_evaluation_results")
        
        try:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            filename = f"{timestamp}.json"
            filepath = self.results_dir / filename
            
            # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
            evaluation_report = {
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "dataset_version": dataset["metadata"]["version"],
                    "model_config": {
                        "llm_model": "solar-pro2",
                        "embedding_model": "embedding-query",
                        "chunk_size": 1000,
                        "chunk_overlap": 50,
                        "retrieval_k": 5
                    },
                    "evaluation_framework": "RAGAS 0.3.2"
                },
                "overall_scores": overall_scores,
                "category_scores": category_scores,
                "detailed_results": results,
                "summary": {
                    "total_questions": len(results),
                    "total_processing_time_ms": sum(r.get("processing_time_ms", 0) for r in results),
                    "avg_processing_time_ms": sum(r.get("processing_time_ms", 0) for r in results) / len(results),
                    "memory_test_count": len([r for r in results if r["category"] == "memory"]),
                    "error_count": len([r for r in results if r.get("error")])
                }
            }
            
            # ê²°ê³¼ ì €ì¥
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(evaluation_report, f, ensure_ascii=False, indent=2)
            
            # latest.json ì‹¬ë³¼ë¦­ ë§í¬ ì—…ë°ì´íŠ¸
            latest_path = self.results_dir / "latest.json"
            if latest_path.exists():
                latest_path.unlink()
            
            # ìƒëŒ€ ê²½ë¡œë¡œ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
            latest_path.symlink_to(filename)
            
            self.logger.log_function_end("save_evaluation_results", f"ê²°ê³¼ ì €ì¥: {filepath}")
            return str(filepath)
            
        except Exception as e:
            self.logger.log_error("save_evaluation_results", e)
            raise
    
    def print_evaluation_summary(self, overall_scores: Dict[str, float], 
                               category_scores: Dict[str, Any], 
                               results: List[Dict[str, Any]]):
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ¯ RAG ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ ê²°ê³¼")
        print("="*60)
        
        # ì „ì²´ ì ìˆ˜
        print("\nğŸ“Š ì „ì²´ RAGAS ì ìˆ˜:")
        print(f"  â€¢ Faithfulness (ì‚¬ì‹¤ ì •í™•ì„±):     {overall_scores.get('faithfulness', 0):.3f}")
        print(f"  â€¢ Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±):  {overall_scores.get('answer_relevancy', 0):.3f}")
        print(f"  â€¢ Context Recall (ì»¨í…ìŠ¤íŠ¸ íšŒìƒë¥ ): {overall_scores.get('context_recall', 0):.3f}")
        print(f"  â€¢ Answer Correctness (ë‹µë³€ ì •í™•ì„±): {overall_scores.get('answer_correctness', 0):.3f}")
        print(f"  â€¢ ğŸ“ˆ RAGAS ì¢…í•© ì ìˆ˜:            {overall_scores.get('ragas_score', 0):.3f}")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜
        print("\nğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„:")
        for category, scores in category_scores.items():
            print(f"  â€¢ {category.upper()}: {scores['question_count']}ê°œ ì§ˆë¬¸")
            print(f"    - ì„±ê³µë¥ : {scores['success_rate']:.1%}")
            print(f"    - í‰ê·  ì²˜ë¦¬ì‹œê°„: {scores['avg_processing_time_ms']:.0f}ms")
        
        # ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        memory_questions = [r for r in results if r["category"] == "memory"]
        if memory_questions:
            memory_success = len([r for r in memory_questions if not r.get("error")])
            print(f"\nğŸ§  ë©”ëª¨ë¦¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸: {memory_success}/{len(memory_questions)} ì„±ê³µ")
        
        # ì²˜ë¦¬ í†µê³„
        total_time = sum(r.get("processing_time_ms", 0) for r in results)
        avg_time = total_time / len(results) if results else 0
        print(f"\nâ±ï¸  ì²˜ë¦¬ ì‹œê°„ í†µê³„:")
        print(f"  â€¢ ì´ ì²˜ë¦¬ì‹œê°„: {total_time:.0f}ms")
        print(f"  â€¢ í‰ê·  ì²˜ë¦¬ì‹œê°„: {avg_time:.0f}ms")
        
        print("\n" + "="*60)
    
    def run_evaluation(self):
        """ì „ì²´ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        self.logger.log_success("=== RAG í’ˆì§ˆ í‰ê°€ ì‹œì‘ ===")
        
        try:
            # 1. ë°ì´í„°ì…‹ ë¡œë“œ
            dataset = self.load_evaluation_dataset()
            
            # 2. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            if not self.initialize_system():
                self.logger.log_error_with_icon("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False
            
            # 3. ì§ˆë¬¸ ì²˜ë¦¬ ë° ë‹µë³€ ìƒì„±
            results = self.process_questions(dataset)
            
            # 4. RAGAS í‰ê°€ ì‹¤í–‰
            overall_scores = self.run_ragas_evaluation(results)
            
            # 5. ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
            category_scores = self.calculate_category_scores(results, overall_scores)
            
            # 6. ê²°ê³¼ ì €ì¥
            result_file = self.save_evaluation_results(dataset, results, overall_scores, category_scores)
            
            # 7. ê²°ê³¼ ì¶œë ¥
            self.print_evaluation_summary(overall_scores, category_scores, results)
            
            print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {result_file}")
            print(f"ğŸ“‹ ìµœì‹  ê²°ê³¼ í™•ì¸: {self.results_dir}/latest.json")
            
            self.logger.log_success("=== RAG í’ˆì§ˆ í‰ê°€ ì™„ë£Œ ===")
            return True
            
        except Exception as e:
            self.logger.log_error("run_evaluation", e)
            print(f"\nâŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ RAG ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ CLI")
    print("WebUIì™€ ë™ì¼í•œ RAG ë°©ì‹ìœ¼ë¡œ RAGAS ë©”íŠ¸ë¦­ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n")
    
    evaluator = RAGEvaluator()
    success = evaluator.run_evaluation()
    
    if success:
        print("\nâœ… í‰ê°€ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return 0
    else:
        print("\nâŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        return 1


if __name__ == "__main__":
    exit(main())