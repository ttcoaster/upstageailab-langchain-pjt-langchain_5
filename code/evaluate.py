"""
RAG ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ ë„êµ¬ (evaluate.py)

WebUIì™€ ë™ì¼í•œ RAG ë°©ì‹ ë° ë©”ëª¨ë¦¬ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬
RAGAS í”„ë ˆì„ì›Œí¬ë¡œ ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- WebUIì™€ ë™ì¼í•œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (VectorStoreManager, LLMManager, RetrieverManager, ChatHistoryManager)
- ì‚¬ì „ ì •ì˜ëœ ì§ˆë¬¸-ì •ë‹µ ë°ì´í„°ì…‹ ì‚¬ìš© (data/eval/question_dataset.json)
- RAGAS ë©”íŠ¸ë¦­ì„ í†µí•œ í’ˆì§ˆ í‰ê°€ (faithfulness, answer_relevancy, context_recall, answer_correctness)
- í‰ê°€ ê²°ê³¼ ì €ì¥ ë° ë¦¬í¬íŠ¸ ìƒì„± (data/eval/evaluation_results/)
- Upstage API ì§ì ‘ ì‚¬ìš©ìœ¼ë¡œ RAGAS í˜¸í™˜ì„± í™•ë³´

ì‚¬ìš©ë²•:
    uv run python code/evaluate.py

"""

import os
import sys
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Tuple

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
script_dir = Path(__file__).parent.absolute()
if str(script_dir) not in sys.path:
    sys.path.insert(0, str(script_dir))

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv(script_dir / '.env')

# LangChain ë° Upstage imports
from langchain_upstage import UpstageEmbeddings

# RAGAS imports (Upstage API ì‚¬ìš©)
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy, 
    context_recall,
    answer_correctness
)
from datasets import Dataset

# ëª¨ë“ˆ imports
from modules import (
    VectorStoreManager, LLMManager, RetrieverManager, 
    ChatHistoryManager, LoggerManager, RAGSystemInitializer, RAGQueryProcessor
)


# ì „ì—­ ì„¤ì •
project_root = script_dir.parent
dataset_path = project_root / "data" / "eval" / "question_dataset.json"
results_dir = project_root / "data" / "eval" / "evaluation_results"

def setup_upstage_for_ragas():
    """RAGASì—ì„œ Upstage APIë¥¼ ì‚¬ìš©í•˜ë„ë¡ í™˜ê²½ë³€ìˆ˜ ì„¤ì •"""
    upstage_api_key = os.getenv("UPSTAGE_API_KEY")
    if upstage_api_key:
        os.environ["OPENAI_API_KEY"] = upstage_api_key
        os.environ["OPENAI_BASE_URL"] = "https://api.upstage.ai/v1"
        os.environ["OPENAI_MODEL_NAME"] = "solar-pro2"




class RAGEvaluator:
    """RAG ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ë¥¼ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = LoggerManager("RAGEvaluator")
        self.results_dir = results_dir
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        self.results_dir.mkdir(parents=True, exist_ok=True)
    
    def load_evaluation_dataset(self) -> Dict[str, Any]:
        """í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ"""
        with open(dataset_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def initialize_system(self):
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        result = RAGSystemInitializer.initialize_system(
            current_file_path=script_dir,
            include_sql=False,
            logger_name="EvaluationRAG",
            enable_db_memory=False  # í‰ê°€ìš©ì€ ë©”ëª¨ë¦¬ë§Œ ì‚¬ìš©
        )
        
        if result is None:
            return False
        
        self.vector_manager, self.llm_manager, self.retriever_manager, self.query_processor = result
        return True
    
    def process_questions(self, dataset: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì§ˆë¬¸ë“¤ì„ ì²˜ë¦¬í•˜ê³  ë‹µë³€ ìƒì„±"""
        results = []
        questions = dataset["questions"]
        
        for i, question_data in enumerate(questions, 1):
            start_time = time.time()
            
            try:
                question = question_data["question"]
                
                # ìë™ ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì´ í¬í•¨ëœ ê°„ë‹¨í•œ ì§ˆì˜ ì²˜ë¦¬ ì‚¬ìš©
                result = self.query_processor.query(
                    question=question,
                    return_sources=True
                )
                
                if not result["success"]:
                    raise Exception(result["error"])
                
                response = result["response"]
                documents = result.get("documents", [])
                source_documents = result.get("sources", [])
                contexts = [doc.page_content for doc in documents]
                
                processing_time = (time.time() - start_time) * 1000  # ms
                
                # ê²°ê³¼ ì €ì¥
                results.append({
                    "question_id": question_data["id"],
                    "question": question,
                    "generated_answer": response,
                    "ground_truth": question_data["ground_truth"],
                    "category": question_data["category"],
                    "difficulty": question_data["difficulty"],
                    "retrieved_contexts": contexts,
                    "source_documents": source_documents,
                    "expected_sources": question_data.get("expected_sources", []),
                    "processing_time_ms": round(processing_time, 2),
                    "depends_on": question_data.get("depends_on"),
                    "keywords": question_data.get("keywords", [])
                })
                
            except Exception as e:
                # ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ë¹ˆ ê²°ê³¼ ì¶”ê°€
                results.append({
                    "question_id": question_data["id"],
                    "question": question_data["question"],
                    "generated_answer": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                    "ground_truth": question_data["ground_truth"],
                    "category": question_data["category"],
                    "difficulty": question_data["difficulty"],
                    "retrieved_contexts": [],
                    "source_documents": [],
                    "expected_sources": question_data.get("expected_sources", []),
                    "processing_time_ms": 0,
                    "error": str(e)
                })
        
        return results
    
    def run_ragas_evaluation(self, results: List[Dict[str, Any]]) -> Dict[str, float]:
        """RAGAS ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•œ í‰ê°€ ì‹¤í–‰ (Upstage API ì‚¬ìš©)"""
        self.logger.log_function_start("run_ragas_evaluation")
        
        try:
            # RAGAS í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ êµ¬ì„±
            dataset_dict = {
                "question": [r["question"] for r in results],
                "answer": [r["generated_answer"] for r in results],
                "contexts": [r["retrieved_contexts"] for r in results],
                "ground_truth": [r["ground_truth"] for r in results]
            }
            
            # Dataset ê°ì²´ ìƒì„±
            dataset = Dataset.from_dict(dataset_dict)
            
            self.logger.log_step("RAGAS í‰ê°€ ì‹¤í–‰", "ë©”íŠ¸ë¦­: faithfulness, answer_relevancy, context_recall, answer_correctness (Upstage API ì‚¬ìš©)")
            
            # baseline.py ë°©ì‹ìœ¼ë¡œ Upstage ëª¨ë¸ ì§ì ‘ ì‚¬ìš©
            from langchain_upstage import ChatUpstage, UpstageEmbeddings
            
            upstage_llm = ChatUpstage(
                api_key=os.getenv("UPSTAGE_API_KEY"),
                model="solar-pro2",
                reasoning_effort="high"
            )
            
            upstage_embeddings = UpstageEmbeddings(
                api_key=os.getenv("UPSTAGE_API_KEY"),
                model="embedding-query"
            )
            
            # RAGAS í‰ê°€ ì‹¤í–‰ (baseline.pyì™€ ë™ì¼í•œ Upstage ëª¨ë¸ ì‚¬ìš©)
            evaluation_result = evaluate(
                dataset=dataset,
                metrics=[faithfulness, answer_relevancy, context_recall, answer_correctness],
                llm=upstage_llm,
                embeddings=upstage_embeddings
            )
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (baseline.py ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •)
            scores = {}
            scores_dict = evaluation_result._scores_dict
            
            for metric_name in ["faithfulness", "answer_relevancy", "context_recall", "answer_correctness"]:
                if metric_name in scores_dict and len(scores_dict[metric_name]) > 0:
                    # ë¦¬ìŠ¤íŠ¸ì˜ í‰ê· ê°’ ê³„ì‚°
                    score_values = scores_dict[metric_name]
                    if isinstance(score_values, list):
                        # numpy scalarì´ë‚˜ float ì²˜ë¦¬
                        avg_score = sum(float(v.item()) if hasattr(v, 'item') else float(v) for v in score_values) / len(score_values)
                        scores[metric_name] = avg_score
                    else:
                        scores[metric_name] = float(score_values.item()) if hasattr(score_values, 'item') else float(score_values)
                else:
                    scores[metric_name] = 0.0
            
            # RAGAS ì¢…í•© ì ìˆ˜ ê³„ì‚° (í‰ê· )
            scores["ragas_score"] = sum(scores.values()) / len(scores)
            
            self.logger.log_function_end("run_ragas_evaluation", f"í‰ê°€ ì™„ë£Œ: {scores['ragas_score']:.3f}")
            return scores
            
        except Exception as e:
            self.logger.log_error("run_ragas_evaluation", e)
            # í‰ê°€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "faithfulness": 0.0,
                "answer_relevancy": 0.0,
                "context_recall": 0.0,
                "answer_correctness": 0.0,
                "ragas_score": 0.0,
                "error": str(e)
            }
    
    def calculate_category_scores(self, results: List[Dict[str, Any]], 
                                overall_scores: Dict[str, float]) -> Dict[str, Any]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°"""
        self.logger.log_function_start("calculate_category_scores")
        
        categories = {}
        
        for result in results:
            category = result["category"]
            if category not in categories:
                categories[category] = {
                    "questions": [],
                    "question_count": 0
                }
            
            categories[category]["questions"].append(result)
            categories[category]["question_count"] += 1
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ (ì „ì²´ ì ìˆ˜ ê¸°ë°˜ ì¶”ì •)
        category_scores = {}
        for category, data in categories.items():
            category_scores[category] = {
                "question_count": data["question_count"],
                "avg_processing_time_ms": sum(q.get("processing_time_ms", 0) 
                                            for q in data["questions"]) / data["question_count"],
                "success_rate": len([q for q in data["questions"] 
                                   if not q.get("error")]) / data["question_count"]
            }
        
        self.logger.log_function_end("calculate_category_scores", 
                                   f"{len(categories)}ê°œ ì¹´í…Œê³ ë¦¬ ë¶„ì„")
        return category_scores
    
    def save_evaluation_results(self, dataset: Dict[str, Any], results: List[Dict[str, Any]], 
                              overall_scores: Dict[str, float], category_scores: Dict[str, Any]) -> str:
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        self.logger.log_function_start("save_evaluation_results")
        
        try:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            filename = f"{timestamp}.json"
            filepath = self.results_dir / filename
            
            # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
            evaluation_report = {
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "dataset_version": dataset["metadata"]["version"],
                    "model_config": {
                        "llm_model": "solar-pro2",
                        "embedding_model": "embedding-query",
                        "chunk_size": 1000,
                        "chunk_overlap": 50,
                        "retrieval_k": 5
                    },
                    "evaluation_framework": "RAGAS 0.3.2"
                },
                "overall_scores": overall_scores,
                "category_scores": category_scores,
                "detailed_results": results,
                "summary": {
                    "total_questions": len(results),
                    "total_processing_time_ms": sum(r.get("processing_time_ms", 0) for r in results),
                    "avg_processing_time_ms": sum(r.get("processing_time_ms", 0) for r in results) / len(results),
                    "memory_test_count": len([r for r in results if r["category"] == "memory"]),
                    "error_count": len([r for r in results if r.get("error")])
                }
            }
            
            # ê²°ê³¼ ì €ì¥
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(evaluation_report, f, ensure_ascii=False, indent=2)
            
            # latest.json ì‹¬ë³¼ë¦­ ë§í¬ ì—…ë°ì´íŠ¸
            latest_path = self.results_dir / "latest.json"
            if latest_path.exists():
                latest_path.unlink()
            
            # ìƒëŒ€ ê²½ë¡œë¡œ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
            latest_path.symlink_to(filename)
            
            self.logger.log_function_end("save_evaluation_results", f"ê²°ê³¼ ì €ì¥: {filepath}")
            return str(filepath)
            
        except Exception as e:
            self.logger.log_error("save_evaluation_results", e)
            raise
    
    def print_evaluation_summary(self, overall_scores: Dict[str, float], 
                               category_scores: Dict[str, Any], 
                               results: List[Dict[str, Any]]):
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ¯ RAG ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ ê²°ê³¼")
        print("="*60)
        
        # ì „ì²´ ì ìˆ˜
        print("\nğŸ“Š ì „ì²´ RAGAS ì ìˆ˜:")
        print(f"  â€¢ Faithfulness (ì‚¬ì‹¤ ì •í™•ì„±):     {overall_scores.get('faithfulness', 0):.3f}")
        print(f"  â€¢ Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±):  {overall_scores.get('answer_relevancy', 0):.3f}")
        print(f"  â€¢ Context Recall (ì»¨í…ìŠ¤íŠ¸ íšŒìƒë¥ ): {overall_scores.get('context_recall', 0):.3f}")
        print(f"  â€¢ Answer Correctness (ë‹µë³€ ì •í™•ì„±): {overall_scores.get('answer_correctness', 0):.3f}")
        print(f"  â€¢ ğŸ“ˆ RAGAS ì¢…í•© ì ìˆ˜:            {overall_scores.get('ragas_score', 0):.3f}")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜
        print("\nğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„:")
        for category, scores in category_scores.items():
            print(f"  â€¢ {category.upper()}: {scores['question_count']}ê°œ ì§ˆë¬¸")
            print(f"    - ì„±ê³µë¥ : {scores['success_rate']:.1%}")
            print(f"    - í‰ê·  ì²˜ë¦¬ì‹œê°„: {scores['avg_processing_time_ms']:.0f}ms")
        
        # ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        memory_questions = [r for r in results if r["category"] == "memory"]
        if memory_questions:
            memory_success = len([r for r in memory_questions if not r.get("error")])
            print(f"\nğŸ§  ë©”ëª¨ë¦¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸: {memory_success}/{len(memory_questions)} ì„±ê³µ")
        
        # ì²˜ë¦¬ í†µê³„
        total_time = sum(r.get("processing_time_ms", 0) for r in results)
        avg_time = total_time / len(results) if results else 0
        print(f"\nâ±ï¸  ì²˜ë¦¬ ì‹œê°„ í†µê³„:")
        print(f"  â€¢ ì´ ì²˜ë¦¬ì‹œê°„: {total_time:.0f}ms")
        print(f"  â€¢ í‰ê·  ì²˜ë¦¬ì‹œê°„: {avg_time:.0f}ms")
        
        print("\n" + "="*60)
    
    def run_evaluation(self):
        """ì „ì²´ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        self.logger.log_success("=== RAG í’ˆì§ˆ í‰ê°€ ì‹œì‘ ===")
        
        try:
            # 1. ë°ì´í„°ì…‹ ë¡œë“œ
            dataset = self.load_evaluation_dataset()
            
            # 2. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            if not self.initialize_system():
                self.logger.log_error_with_icon("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False
            
            # 3. ì§ˆë¬¸ ì²˜ë¦¬ ë° ë‹µë³€ ìƒì„±
            results = self.process_questions(dataset)
            
            # 4. RAGAS í‰ê°€ ì‹¤í–‰
            overall_scores = self.run_ragas_evaluation(results)
            
            # 5. ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
            category_scores = self.calculate_category_scores(results, overall_scores)
            
            # 6. ê²°ê³¼ ì €ì¥
            result_file = self.save_evaluation_results(dataset, results, overall_scores, category_scores)
            
            # 7. ê²°ê³¼ ì¶œë ¥
            self.print_evaluation_summary(overall_scores, category_scores, results)
            
            print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {result_file}")
            print(f"ğŸ“‹ ìµœì‹  ê²°ê³¼ í™•ì¸: {self.results_dir}/latest.json")
            
            self.logger.log_success("=== RAG í’ˆì§ˆ í‰ê°€ ì™„ë£Œ ===")
            return True
            
        except Exception as e:
            self.logger.log_error("run_evaluation", e)
            print(f"\nâŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ RAG ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ CLI")
    print("WebUIì™€ ë™ì¼í•œ RAG ë°©ì‹ìœ¼ë¡œ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n")
    
    try:
        # RAGAS ì„¤ì •
        setup_upstage_for_ragas()
        
        # í‰ê°€ê¸° ìƒì„± ë° ì‹¤í–‰
        evaluator = RAGEvaluator()
        success = evaluator.run_evaluation()
        
        if success:
            print("\nâœ… í‰ê°€ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return 0
        else:
            print("\nâŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            return 1
            
    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return 1


if __name__ == "__main__":
    exit(main())