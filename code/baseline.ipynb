{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation) 시스템\n",
    "\n",
    "이 노트북은 LangChain과 Upstage API를 사용하여 RAG 시스템을 구현합니다.\n",
    "\n",
    "RAG는 다음 단계로 구성됩니다:\n",
    "1. 문서 로드\n",
    "2. 문서 분할\n",
    "3. 임베딩 생성\n",
    "4. 벡터 데이터베이스 생성\n",
    "5. 검색기 생성\n",
    "6. 프롬프트 생성\n",
    "7. 언어모델 생성\n",
    "8. 체인 생성 및 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설정\n",
    "\n",
    "먼저 Upstage API 키를 설정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upstage API 키를 여기에 입력하세요\n",
    "UPSTAGE_API_KEY = \"your_upstage_api_key_here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_upstage import ChatUpstage, UpstageEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단계 1: 문서 로드 (Load Documents)\n",
    "\n",
    "PDF 파일을 로드합니다. PyMuPDFLoader를 사용하여 PDF 문서를 읽어옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\"../data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"로드된 문서 수: {len(docs)}\")\n",
    "print(f\"첫 번째 문서 미리보기: {docs[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단계 2: 문서 분할 (Split Documents)\n",
    "\n",
    "긴 문서를 작은 청크로 분할합니다. 이는 검색 성능을 향상시키고 모델의 컨텍스트 길이 제한을 해결합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"분할된 문서 청크 수: {len(split_documents)}\")\n",
    "print(f\"첫 번째 청크 미리보기: {split_documents[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단계 3: 임베딩 생성 (Embedding)\n",
    "\n",
    "Upstage의 임베딩 모델을 사용하여 텍스트를 벡터로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = UpstageEmbeddings(\n",
    "    api_key=UPSTAGE_API_KEY,\n",
    "    model=\"embedding-query\"\n",
    ")\n",
    "\n",
    "print(\"임베딩 모델이 성공적으로 초기화되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단계 4: 벡터 데이터베이스 생성 및 검색기 설정\n",
    "\n",
    "FAISS를 사용하여 벡터 데이터베이스를 생성하고 검색기를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터스토어 생성\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# 검색기 생성\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(\"벡터 데이터베이스와 검색기가 성공적으로 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단계 5: 프롬프트 생성 (Create Prompt)\n",
    "\n",
    "질문 답변을 위한 프롬프트 템플릿을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean.\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "print(\"프롬프트 템플릿이 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단계 6: 언어모델 생성 (LLM)\n",
    "\n",
    "Upstage의 Solar Pro 모델을 사용하여 언어모델을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatUpstage(\n",
    "    api_key=UPSTAGE_API_KEY,\n",
    "    model=\"solar-pro2\",\n",
    "    reasoning_effort=\"high\"\n",
    ")\n",
    "\n",
    "print(\"언어모델이 성공적으로 초기화되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단계 7: 체인 생성 (Create Chain)\n",
    "\n",
    "검색기, 프롬프트, 언어모델을 연결하여 RAG 체인을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG 체인이 성공적으로 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단계 8: 질문 실행 (Run Chain)\n",
    "\n",
    "문서에 대한 질의를 입력하고 답변을 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 설정\n",
    "question = \"삼성전자가 자체 개발한 AI 의 이름은?\"\n",
    "\n",
    "# 체인 실행\n",
    "response = chain.invoke(question)\n",
    "\n",
    "print(f\"질문: {question}\")\n",
    "print(f\"답변: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가 질문 테스트\n",
    "\n",
    "다른 질문들도 테스트해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 질문으로 테스트\n",
    "new_question = \"AI 기술의 최신 동향은 무엇인가요?\"\n",
    "new_response = chain.invoke(new_question)\n",
    "\n",
    "print(f\"질문: {new_question}\")\n",
    "print(f\"답변: {new_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
