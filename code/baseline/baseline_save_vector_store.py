import os
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferWindowMemory
from langchain_upstage import ChatUpstage
from langchain_upstage import UpstageEmbeddings
 
# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ë¥¼ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
from pathlib import Path
script_dir = Path(__file__).parent.absolute()
os.chdir(script_dir)

import sys; sys.path.append('../');
import modules.logger as log
from vector_store import VectorStoreManager

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
log.info("í™˜ê²½ë³€ìˆ˜ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")


# ë‹¨ê³„ 1-3: ì„ë² ë”©(Embedding) ìƒì„±
log.info("ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘...")
embeddings = UpstageEmbeddings(
    api_key=os.getenv("UPSTAGE_API_KEY"),
    model="embedding-query"
)
log.info("ì„ë² ë”© ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë‹¨ê³„ 4: ë²¡í„°ìŠ¤í† ì–´ ê´€ë¦¬ì ìƒì„± ë° ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ/ìƒì„±
log.info("ë²¡í„°ìŠ¤í† ì–´ ê´€ë¦¬ìë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘...")
vector_manager = VectorStoreManager(
    pdf_dir="../../data/pdf",
    vectorstore_dir="../../data/vectorstore", 
    embeddings=embeddings,
    chunk_size=1000,
    chunk_overlap=50
)

# ë²¡í„°ìŠ¤í† ì–´ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±, ì¦ë¶„ ì—…ë°ì´íŠ¸ ìë™ ì²˜ë¦¬)
vectorstore = vector_manager.get_or_create_vectorstore()

if vectorstore is None:
    log.error("ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

# ë‹¨ê³„ 5: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±
retriever = vectorstore.as_retriever()
log.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ ê²€ìƒ‰ê¸°ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë‹¨ê³„ 6: í”„ë¡¬í”„íŠ¸ ìƒì„±(Create Prompt)
# ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
prompt = ChatPromptTemplate.from_messages([
    ("system", """You are an assistant for question-answering tasks. 
Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. 
Answer in Korean.

Context: {context}"""),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{question}")
])
log.info("í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë‹¨ê³„ 7: ì–¸ì–´ëª¨ë¸(LLM) ìƒì„±
log.info("ì–¸ì–´ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘...")
llm = ChatUpstage(
    api_key=os.getenv("UPSTAGE_API_KEY"),
    model="solar-pro2",
    reasoning_effort="high"
)
log.info("ì–¸ì–´ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë‹¨ê³„ 7-1: ë©”ëª¨ë¦¬ ìƒì„± (ìµœê·¼ 3ê°œ ëŒ€í™” ì €ì¥)
memory = ConversationBufferWindowMemory(
    k=3,  # ìµœê·¼ 3ê°œ ëŒ€í™”ë§Œ ê¸°ì–µ
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)


# ë‹¨ê³„ 8: ì²´ì¸(Chain) ìƒì„± - ë©”ëª¨ë¦¬ ê¸°ëŠ¥ í¬í•¨
def create_rag_chain_with_memory():
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    # RAG ì²´ì¸ê³¼ ë©”ëª¨ë¦¬ë¥¼ ê²°í•©í•˜ëŠ” í•¨ìˆ˜
    def rag_with_memory(question):
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        docs = retriever.invoke(question)
        context = format_docs(docs)
        
        # ë©”ëª¨ë¦¬ì—ì„œ ì±„íŒ… ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        chat_history = memory.chat_memory.messages
        
        # í”„ë¡¬í”„íŠ¸ì— í•„ìš”í•œ ë³€ìˆ˜ë“¤ ì¤€ë¹„
        formatted_prompt = prompt.format_messages(
            context=context,
            chat_history=chat_history,
            question=question
        )
        
        # LLM í˜¸ì¶œ
        response = llm.invoke(formatted_prompt)
        
        # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
        memory.save_context({"input": question}, {"answer": response.content})
        
        return response.content
    
    return rag_with_memory

# ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì´ í¬í•¨ëœ RAG ì²´ì¸ ìƒì„±
chain = create_rag_chain_with_memory()
log.info("RAG ì²´ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì²´ì¸ ì‹¤í–‰(Run Chain) - ë©”ëª¨ë¦¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
# 5ê°œ ì§ˆë¬¸ìœ¼ë¡œ ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

questions = [
    "ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ AIì˜ ì´ë¦„ì€?",
    "AI ë¸Œë¦¬í”„ëŠ” ì–¸ì œ ë°œí–‰ë˜ë‚˜ìš”?", 
    "ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”",
    "ë„¤ ë²ˆì§¸ ì§ˆë¬¸ì´ì•¼. ì²˜ìŒì— ë‚´ê°€ ë­˜ ë¬¼ì–´ë´¤ëŠ”ì§€ ê¸°ì–µí•´?",
    "ì²« ë²ˆì§¸ ì§ˆë¬¸ê³¼ ë‘ ë²ˆì§¸ ì§ˆë¬¸ì„ ê¸°ì–µí•˜ê³  ìˆì–´?"
]

log.info("=== LangChain ë©”ëª¨ë¦¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
log.info(f"ë©”ëª¨ë¦¬ ì„¤ì •: ìµœê·¼ {memory.k}ê°œ ëŒ€í™” ê¸°ì–µ\n")

for i, question in enumerate(questions, 1):
    log.info(f"ğŸ”µ ì§ˆë¬¸ {i}: {question}")
    
    # ì§ˆë¬¸ ì‹¤í–‰
    response = chain(question)
    log.info(f"ğŸ’¬ ë‹µë³€: {response}")
    
    # í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    log.info(f"ğŸ“ í˜„ì¬ ë©”ëª¨ë¦¬ì— ì €ì¥ëœ ëŒ€í™” ìˆ˜: {len(memory.chat_memory.messages) // 2}")
    log.info("-" * 80)