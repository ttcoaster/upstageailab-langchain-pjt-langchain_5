import os
from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferWindowMemory
from langchain_upstage import ChatUpstage
from langchain_upstage import UpstageEmbeddings
 
# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ë¥¼ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
from pathlib import Path
script_dir = Path(__file__).parent.absolute()
os.chdir(script_dir)

import sys; sys.path.append('../'); import modules.logger as log

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
log.info("í™˜ê²½ë³€ìˆ˜ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")


# ë‹¨ê³„ 1: ë¬¸ì„œ ë¡œë“œ(Load Documents)
log.info("PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
loader = PyMuPDFLoader("../../data/pdf/SPRI_AI_Brief_2023á„‚á…§á†«12á„‹á…¯á†¯á„’á…©_F.pdf")
# loader = PyMuPDFLoader("../../data/pdf/4.á„ƒá…¡á†«á„‘á…¡á‡€á„ˆá…¡á†¼(á„‡á…µá„‰á…¡á†¼á„‰á…³á„á…³á„…á…¦á„‹á…µá„á…³á„‡á…¥á†¸).pdf")
docs = loader.load()
log.info(f"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
log.info(f"ì²« ë²ˆì§¸ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°: {docs[0].page_content[:200]}...")

# ë‹¨ê³„ 2: ë¬¸ì„œ ë¶„í• (Split Documents)
log.info("ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” ì¤‘...")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
split_documents = text_splitter.split_documents(docs)
log.info(f"ë¶„í• ëœ ë¬¸ì„œ ì²­í¬ ìˆ˜: {len(split_documents)}")
log.info(f"ì²« ë²ˆì§¸ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°: {split_documents[0].page_content[:200]}...")

# ë‹¨ê³„ 3: ì„ë² ë”©(Embedding) ìƒì„±
log.info("ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘...")
# embeddings = OpenAIEmbeddings()
embeddings = UpstageEmbeddings(
    api_key=os.getenv("UPSTAGE_API_KEY"),
    model="embedding-query"
)
log.info("ì„ë² ë”© ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë‹¨ê³„ 4: DB ìƒì„±(Create DB) ë° ì €ì¥
log.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘...")
# ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)

# ë‹¨ê³„ 5: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±
# ë¬¸ì„œì— í¬í•¨ë˜ì–´ ìˆëŠ” ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìƒì„±í•©ë‹ˆë‹¤.
retriever = vectorstore.as_retriever()
log.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ ê²€ìƒ‰ê¸°ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë‹¨ê³„ 6: í”„ë¡¬í”„íŠ¸ ìƒì„±(Create Prompt)
# ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
prompt = ChatPromptTemplate.from_messages([
    ("system", """You are an assistant for question-answering tasks. 
Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. 
Answer in Korean.

Context: {context}"""),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{question}")
])
log.info("í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë‹¨ê³„ 7: ì–¸ì–´ëª¨ë¸(LLM) ìƒì„±
log.info("ì–¸ì–´ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘...")
# ëª¨ë¸(LLM) ì„ ìƒì„±í•©ë‹ˆë‹¤.
# llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
llm = ChatUpstage(
    api_key=os.getenv("UPSTAGE_API_KEY"),
    model="solar-pro2",
    reasoning_effort="high"
)
log.info("ì–¸ì–´ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë‹¨ê³„ 7-1: ë©”ëª¨ë¦¬ ìƒì„± (ìµœê·¼ 3ê°œ ëŒ€í™” ì €ì¥)
memory = ConversationBufferWindowMemory(
    k=3,  # ìµœê·¼ 3ê°œ ëŒ€í™”ë§Œ ê¸°ì–µ
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)


# ë‹¨ê³„ 8: ì²´ì¸(Chain) ìƒì„± - ë©”ëª¨ë¦¬ ê¸°ëŠ¥ í¬í•¨
def create_rag_chain_with_memory():
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    # RAG ì²´ì¸ê³¼ ë©”ëª¨ë¦¬ë¥¼ ê²°í•©í•˜ëŠ” í•¨ìˆ˜
    def rag_with_memory(question):
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        docs = retriever.invoke(question)
        context = format_docs(docs)
        
        # ë©”ëª¨ë¦¬ì—ì„œ ì±„íŒ… ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        chat_history = memory.chat_memory.messages
        
        # í”„ë¡¬í”„íŠ¸ì— í•„ìš”í•œ ë³€ìˆ˜ë“¤ ì¤€ë¹„
        formatted_prompt = prompt.format_messages(
            context=context,
            chat_history=chat_history,
            question=question
        )
        
        # LLM í˜¸ì¶œ
        response = llm.invoke(formatted_prompt)
        
        # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
        memory.save_context({"input": question}, {"answer": response.content})
        
        return response.content
    
    return rag_with_memory

# ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì´ í¬í•¨ëœ RAG ì²´ì¸ ìƒì„±
chain = create_rag_chain_with_memory()
log.info("RAG ì²´ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì²´ì¸ ì‹¤í–‰(Run Chain) - ë©”ëª¨ë¦¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
# 5ê°œ ì§ˆë¬¸ìœ¼ë¡œ ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

questions = [
    "ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ AIì˜ ì´ë¦„ì€?",
    "AI ë¸Œë¦¬í”„ëŠ” ì–¸ì œ ë°œí–‰ë˜ë‚˜ìš”?", 
    "ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”",
    "ë„¤ ë²ˆì§¸ ì§ˆë¬¸ì´ì•¼. ì²˜ìŒì— ë‚´ê°€ ë­˜ ë¬¼ì–´ë´¤ëŠ”ì§€ ê¸°ì–µí•´?",
    "ì²« ë²ˆì§¸ ì§ˆë¬¸ê³¼ ë‘ ë²ˆì§¸ ì§ˆë¬¸ì„ ê¸°ì–µí•˜ê³  ìˆì–´?"
]

log.info("=== LangChain ë©”ëª¨ë¦¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
log.info(f"ë©”ëª¨ë¦¬ ì„¤ì •: ìµœê·¼ {memory.k}ê°œ ëŒ€í™” ê¸°ì–µ\n")

for i, question in enumerate(questions, 1):
    log.info(f"ğŸ”µ ì§ˆë¬¸ {i}: {question}")
    
    # ì§ˆë¬¸ ì‹¤í–‰
    response = chain(question)
    log.info(f"ğŸ’¬ ë‹µë³€: {response}")
    
    # í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    log.info(f"ğŸ“ í˜„ì¬ ë©”ëª¨ë¦¬ì— ì €ì¥ëœ ëŒ€í™” ìˆ˜: {len(memory.chat_memory.messages) // 2}")
    log.info("-" * 80)